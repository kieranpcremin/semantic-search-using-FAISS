# Semantic Search Engine for Technical Documents

**A semantic search engine that understands the *meaning* of your queries ‚Äî not just the keywords ‚Äî to find the most relevant sections across technical document collections, built with SentenceTransformers and FAISS.**

[![Python](https://img.shields.io/badge/Python-3.10+-blue)](https://www.python.org/)
[![SentenceTransformers](https://img.shields.io/badge/SentenceTransformers-2.2+-green)](https://www.sbert.net/)
[![FAISS](https://img.shields.io/badge/FAISS-1.7+-orange)](https://github.com/facebookresearch/faiss)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-FF4B4B)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)

---

<img width="1862" height="967" alt="image" src="https://github.com/user-attachments/assets/0ebc424c-f853-496b-b258-e9ef14dc6170" />


## üéØ What This Project Does

Type a natural language query ‚Äî in your own words ‚Äî and get the most relevant paragraphs from a collection of technical documents, ranked by semantic similarity.

- **Semantic search** ‚Äî finds results by meaning, not keyword matching
- **Cross-document discovery** ‚Äî surfaces related content across different documents you didn't know were connected
- **Upload & search** ‚Äî add your own documents (.md, .txt, .pdf) through the web UI
- **RAG-ready** ‚Äî implements the retrieval half of Retrieval-Augmented Generation

---

## üñ•Ô∏è Try These Searches

| Query | What It Finds |
|-------|--------------|
| `fire resistance requirements for steel structures` | Results from **both** fire protection and structural steel docs |
| `personal protective equipment compliance` | Safety standards content ‚Äî even paragraphs that say "hard hats" not "PPE" |
| `hazardous waste disposal procedures` | Environmental compliance content |
| `lockout tagout electrical safety` | Electrical safety procedures |
| `stormwater management and erosion control` | Environmental content about water management |

---

## ü§î The Problem: Why Keyword Search Fails

Traditional keyword search (like `Ctrl+F` or SQL `LIKE '%term%'`) has fundamental limitations when searching technical documents:

### 1. The Synonym Problem
A construction engineer searching for **"PPE requirements"** won't find paragraphs that say *"hard hats, safety glasses, and high-visibility vests must be worn"* ‚Äî even though that's exactly what they need. Keyword search demands the exact term.

### 2. The Context Problem
Searching for **"fire resistance"** in a structural steel document and a fire protection document requires understanding that both topics are relevant. A keyword search treats each match identically ‚Äî it has no concept of *relevance* or *meaning*.

### 3. The Vocabulary Mismatch Problem
Technical documents use varied terminology. **"Fall protection"** might appear as *"fall arrest systems"*, *"guardrails"*, *"safety harnesses"*, or *"edge protection"*. A keyword search needs every variant spelled out; semantic search handles this automatically.

### 4. The Natural Language Problem
Engineers ask questions in natural language: *"What are the requirements for working at height?"*. Keyword search can't interpret this as a question ‚Äî it just looks for the literal words "requirements", "working", "height" with no understanding of intent.

### How Semantic Search Solves This

Instead of matching words, semantic search converts both the query and every document chunk into **embedding vectors** ‚Äî numerical representations that capture meaning. Similar meanings produce similar vectors, regardless of the exact words used.

```
Query: "safety gear for workers"         Document chunk: "PPE including hard hats
         ‚îÇ                                and high-vis vests are mandatory"
         ‚ñº                                         ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ [0.12,   ‚îÇ    cosine similarity         ‚îÇ [0.15,   ‚îÇ
   ‚îÇ  0.84,   ‚îÇ ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ 0.82 (high!) ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫    ‚îÇ  0.79,   ‚îÇ
   ‚îÇ  0.31,   ‚îÇ                              ‚îÇ  0.28,   ‚îÇ
   ‚îÇ  ...384] ‚îÇ                              ‚îÇ  ...384] ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   Query vector                              Document vector
```

The vectors are close in 384-dimensional space because the **meanings** are related, even though the **words** are different.

---

## üß† How Embeddings Work

This is the core concept. If you understand embeddings, you understand modern NLP.

### What Is an Embedding?

An embedding is a list of numbers (a **vector**) that represents the meaning of a piece of text. The model `all-MiniLM-L6-v2` produces 384 numbers for any input text.

```python
"construction safety"  ‚Üí  [0.12, -0.45, 0.78, 0.03, ..., -0.21]  # 384 numbers
"building site hazards" ‚Üí [0.11, -0.42, 0.75, 0.05, ..., -0.19]  # similar numbers!
"Italian cooking recipes" ‚Üí [0.89, 0.12, -0.67, 0.44, ..., 0.56]  # very different
```

### Why Do Similar Meanings Get Similar Vectors?

The model was trained on millions of text pairs where humans labelled whether two sentences mean similar things. Through training, the model learned to place related concepts close together in vector space and unrelated concepts far apart.

### Measuring Similarity: Cosine Similarity

To find how similar two embeddings are, we calculate the **cosine of the angle** between them:

| Score | Meaning |
|-------|---------|
| **1.0** | Identical meaning |
| **0.7‚Äì0.9** | Strongly related |
| **0.4‚Äì0.7** | Somewhat related |
| **0.0‚Äì0.3** | Unrelated |
| **-1.0** | Opposite meaning |

**Implementation trick:** If you normalise vectors to unit length first, cosine similarity becomes a simple dot product ‚Äî which FAISS computes extremely efficiently.

### Why all-MiniLM-L6-v2?

| Property | Value |
|----------|-------|
| **Parameters** | 22 million |
| **Output dimensions** | 384 |
| **Model size** | ~88 MB |
| **Speed** | ~14,000 sentences/second on GPU |
| **Quality** | Excellent for its size ‚Äî trained on 1B+ sentence pairs |
| **Runs on** | CPU or GPU (this project uses CPU) |

It's the sweet spot: small enough to run on any machine, good enough to capture nuanced meaning. Larger models (like `all-mpnet-base-v2` at 768 dimensions) are more accurate but slower and heavier.

---

## üèóÔ∏è Architecture

### System Overview

```
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ           Streamlit Web UI               ‚îÇ
                        ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
                        ‚îÇ  ‚îÇ Search Bar  ‚îÇ  ‚îÇ  File Upload     ‚îÇ  ‚îÇ
                        ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ                   ‚îÇ
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ          SearchPipeline                  ‚îÇ
                        ‚îÇ         (Orchestrator)                   ‚îÇ
                        ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ             ‚îÇ              ‚îÇ
                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                  ‚îÇ Document      ‚îÇ ‚îÇ Embedding ‚îÇ ‚îÇ  VectorStore  ‚îÇ
                  ‚îÇ Processor     ‚îÇ ‚îÇ Model     ‚îÇ ‚îÇ  (FAISS)      ‚îÇ
                  ‚îÇ               ‚îÇ ‚îÇ           ‚îÇ ‚îÇ               ‚îÇ
                  ‚îÇ Load files    ‚îÇ ‚îÇ MiniLM    ‚îÇ ‚îÇ Index vectors ‚îÇ
                  ‚îÇ Chunk text    ‚îÇ ‚îÇ 384-dim   ‚îÇ ‚îÇ Search cosine ‚îÇ
                  ‚îÇ .md .txt .pdf ‚îÇ ‚îÇ vectors   ‚îÇ ‚îÇ Persist disk  ‚îÇ
                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Indexing Flow (One-Time)

```
Documents (.md, .txt, .pdf)
    ‚îÇ
    ‚ñº
1. Load text content
    ‚îÇ
    ‚ñº
2. Split into chunks (500 chars, 50 char overlap)
    ‚îÇ  - Respect paragraph boundaries
    ‚îÇ  - Break long paragraphs by sentence
    ‚îÇ  - Overlap prevents losing context at edges
    ‚îÇ
    ‚ñº
3. Generate embeddings (384-dim vectors per chunk)
    ‚îÇ
    ‚ñº
4. Normalise vectors (unit length for cosine similarity)
    ‚îÇ
    ‚ñº
5. Add to FAISS index + save metadata to disk
```

### Search Flow (Per Query)

```
User query: "fire resistance for steel"
    ‚îÇ
    ‚ñº
1. Embed query ‚Üí 384-dim vector
    ‚îÇ
    ‚ñº
2. Normalise query vector
    ‚îÇ
    ‚ñº
3. FAISS finds top-N closest document vectors (dot product)
    ‚îÇ
    ‚ñº
4. Return chunks + similarity scores
    ‚îÇ
    ‚ñº
5. Display ranked results in UI
```

---

## üìÑ Text Chunking Strategy

Chunking ‚Äî how you split documents into searchable pieces ‚Äî is one of the most important design decisions in a semantic search system.

### Why Chunk at All?

Embedding models have **token limits** (MiniLM maxes out at 256 tokens ‚âà 400 words). Even if they didn't, embedding an entire 10-page document into a single vector would dilute the meaning ‚Äî the vector would be an average of everything, matching nothing well.

### The Chunking Algorithm

```
Step 1: Split on paragraph boundaries (\n\n)
        ‚Üí Preserves logical units of thought

Step 2: If paragraph > 500 chars, split on sentences
        ‚Üí Prevents oversized chunks

Step 3: Combine small segments until chunk_size reached
        ‚Üí Avoids tiny, meaningless chunks

Step 4: Add 50-char overlap between adjacent chunks
        ‚Üí Prevents losing context at boundaries
```

### Chunk Size Trade-offs

| Size | Pros | Cons |
|------|------|------|
| **Too small** (< 200 chars) | Precise matches | Loses context; many fragments |
| **Too large** (> 1000 chars) | Rich context | Diluted embedding; less precise |
| **Sweet spot** (400‚Äì600 chars) | Good precision + context | Requires tuning per domain |

### Why Overlap Matters

Without overlap, a sentence split across two chunks would be lost from both embeddings. A 50-character overlap means the end of chunk N appears at the start of chunk N+1, preserving context at boundaries.

---

## üîç Keyword Search vs Semantic Search

| Feature | Keyword Search | Semantic Search |
|---------|---------------|-----------------|
| **Matching** | Exact word matching | Meaning-based matching |
| **Synonyms** | Misses them ‚Äî "PPE" won't find "safety gear" | Finds them automatically |
| **Context** | No understanding of what words mean together | Understands phrases and intent |
| **Cross-topic** | Only finds documents with exact terms | Surfaces related content across documents |
| **Typo tolerance** | Fails on misspellings | Handles variations naturally |
| **Natural language** | Can't interpret questions | Understands query intent |
| **Setup** | Simple string matching | Requires embedding model + vector store |
| **Speed** | Faster for exact lookups | Slightly slower (embedding computation) |
| **Scalability** | Degrades with corpus size | FAISS handles millions of vectors |

**Example:** Searching **"personal protective equipment"** with keyword search won't find paragraphs that say *"safety gear"*, *"hard hats and safety glasses"*, or *"high-visibility vests must be worn."* Semantic search understands these all refer to the same concept.

---

## üß† Key Concepts Demonstrated

| Concept | Where | What I Learned |
|---------|-------|---------------|
| **Text Embeddings** | `embeddings.py` | Converting text into 384-dimensional vectors that capture meaning, not just words |
| **Vector Similarity** | `vector_store.py` | Using cosine similarity (via normalised dot product) to find semantically related content |
| **Text Chunking** | `document_processor.py` | Splitting documents into searchable chunks with paragraph-aware boundaries and overlap |
| **FAISS Vector Store** | `vector_store.py` | Efficient similarity search with persistence ‚Äî FAISS stores vectors, metadata stored separately |
| **RAG Pipeline** | `search.py` | The retrieval half of Retrieval-Augmented Generation ‚Äî add an LLM and you have a full RAG system |
| **Pipeline Orchestration** | `search.py` | Composing modular components (processor, embeddings, store) into a clean pipeline |
| **Web Deployment** | `streamlit_app.py` | Serving an ML-powered search engine through an interactive UI with file upload |

---

## üìÅ Project Structure

```
semantic-search-using-FAISS/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îî‚îÄ‚îÄ streamlit_app.py           # Web UI ‚Äî search interface + file upload
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py      # Load files (.md, .txt, .pdf) + chunk text
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py              # SentenceTransformer wrapper (all-MiniLM-L6-v2)
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py            # FAISS index ‚Äî store, search, persist vectors
‚îÇ   ‚îî‚îÄ‚îÄ search.py                  # Pipeline orchestrator + SearchResult dataclass
‚îú‚îÄ‚îÄ documents/                     # 5 sample engineering/construction documents
‚îÇ   ‚îú‚îÄ‚îÄ construction_safety_standards.md
‚îÇ   ‚îú‚îÄ‚îÄ structural_steel_requirements.md
‚îÇ   ‚îú‚îÄ‚îÄ fire_protection_guidelines.md
‚îÇ   ‚îú‚îÄ‚îÄ electrical_safety_procedures.md
‚îÇ   ‚îî‚îÄ‚îÄ environmental_compliance.md
‚îú‚îÄ‚îÄ data/                          # FAISS index + metadata (generated, not in repo)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ LICENSE
‚îî‚îÄ‚îÄ README.md
```

---

## üîç Known Limitations & Honest Reflection

| Limitation | Impact | What I'd Do Differently |
|-----------|--------|------------------------|
| **Small corpus** | Only 5 sample documents (62 chunks) ‚Äî too small to stress-test ranking | Test with 100+ real technical documents |
| **No hybrid search** | Pure semantic search misses exact term matches (e.g. part numbers, codes) | Combine with BM25 keyword search (hybrid approach) |
| **Fixed chunk size** | 500-char chunks may split important sections awkwardly | Use recursive or semantic chunking that respects document structure |
| **No re-ranking** | FAISS returns results in one pass ‚Äî no refinement | Add a cross-encoder re-ranker for the top-N results |
| **No evaluation metrics** | No way to measure search quality objectively | Build a test set with labelled relevant results and measure MRR/NDCG |
| **Single embedding model** | all-MiniLM-L6-v2 is general-purpose, not domain-specific | Fine-tune on construction/engineering text or try domain-specific models |

### How I'd Improve It

- ‚úÖ **Hybrid search** ‚Äî combine FAISS semantic search with BM25 keyword search, merge results with reciprocal rank fusion
- ‚úÖ **Cross-encoder re-ranking** ‚Äî use a more expensive model to re-rank just the top 20 results for better precision
- ‚úÖ **Add an LLM** ‚Äî connect to an LLM to build a full RAG system that answers questions using retrieved context
- ‚úÖ **Better chunking** ‚Äî use recursive text splitters that respect markdown headers and section boundaries
- ‚úÖ **Evaluation** ‚Äî create a test set of queries with known relevant documents and measure retrieval quality

> This project implements the **retrieval** half of RAG. The architecture is designed so adding an LLM for the **generation** half would require minimal changes ‚Äî feed the top-N search results as context to the LLM prompt.

---

## üöÄ Setup

### 1. Clone & Create Environment

```bash
git clone https://github.com/kieranpcremin/semantic-search-using-FAISS.git
cd semantic-search-using-FAISS
python -m venv venv
venv\Scripts\activate        # Windows
# source venv/bin/activate   # Mac/Linux
pip install -r requirements.txt
```

> **Note:** The first run downloads the `all-MiniLM-L6-v2` model (~88 MB) from Hugging Face. Subsequent runs use the cached version.

### 2. Index Sample Documents

```bash
python -c "from src.search import SearchPipeline; p = SearchPipeline(); print(f'Indexed {p.index()} chunks')"
```

This processes the 5 sample technical documents, chunks them into ~62 paragraphs, generates embeddings, and stores everything in a FAISS index.

### 3. Run the Web App

```bash
streamlit run app/streamlit_app.py
```

### 4. Upload Your Own Documents

Use the file upload in the sidebar to add your own `.md`, `.txt`, or `.pdf` documents. They're indexed immediately and searchable alongside the sample docs.

---

## üõ†Ô∏è Tech Stack

| Component | Technology | Role |
|-----------|-----------|------|
| **Embeddings** | SentenceTransformers (`all-MiniLM-L6-v2`) | Convert text to 384-dim meaning vectors |
| **Vector Store** | FAISS (`IndexFlatIP`) | Fast cosine similarity search on normalised vectors |
| **Web UI** | Streamlit | Interactive search interface with file upload |
| **PDF Parsing** | PyPDF2 | Extract text from uploaded PDF documents |
| **ML Backend** | PyTorch | Runtime for the SentenceTransformer model |

---

## üìö Data Types And Tech Stacks

| Project | Data Type | ML Type | Key Tech |
|---------|----------|---------|----------|
| [Safety Detector](https://github.com/kieranpcremin/hard-hat-detector) | Images | Classification (CNN) | PyTorch, ResNet18 |
| [Safety Detector (.NET)](https://github.com/kieranpcremin/safety-detector-dotnet) | Images | Classification (CNN) | .NET, TensorFlow, ML.NET |
| **Semantic Search** | **Text** | **Embeddings + Search** | **SentenceTransformers, FAISS** |
| [Timeline Predictor](https://github.com/kieranpcremin/project-timeline-predictor) | Tabular | Regression | scikit-learn, XGBoost |

---

## üë®‚Äçüíª Author

**Kieran Cremin**
Built with assistance from Claude (Anthropic)

---

## üìÑ License

MIT License ‚Äî Free to use, modify, and distribute.
